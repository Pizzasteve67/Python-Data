{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3, 2, 3])\n",
      "torch.Size([3])\n",
      "torch.Size([3, 2])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([3., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#pytorch works with tensor objects\n",
    "#Here is how you initialize tensor objects\n",
    "\n",
    "x=torch.tensor([3,2,3])\n",
    "print(x)\n",
    "print(x.size()) #can print the size of the tensor\n",
    "\n",
    "#randomize\n",
    "y=torch.rand(3,2) #random 3 by 2 matrix\n",
    "print(y.size()) #can print size of tensor as shown here\n",
    "\n",
    "#fill with ones\n",
    "z=torch.ones(3,2)\n",
    "print(z)\n",
    "\n",
    "#fill with zeros\n",
    "f=torch.ones(3,2)\n",
    "print(f)\n",
    "\n",
    "#most of pytorch works with the float32 format\n",
    "#It is best to change the type of the tensor to float32\n",
    "\n",
    "x=torch.tensor([3,2,3],dtype=torch.float32)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[3.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[3., 2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "#you can also change the dimension of the tensor\n",
    "\n",
    "print(x.size())\n",
    "x=x.view(3,1) #change x into a 3 by 1 matrix\n",
    "print(x)\n",
    "\n",
    "x=x.view(1,-1) #can use -1 and pytorch will infer the remaining size based on the other size you gave\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:  0  Weight:  0.7420543432235718  Loss:  16.42658042907715\n",
      "Epoch Number:  50  Weight:  1.999627947807312  Loss:  1.4370993994816672e-06\n",
      "Epoch Number:  100  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  150  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  200  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  250  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  300  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n"
     ]
    }
   ],
   "source": [
    "#Here we will build a simple linear regression model from scratch using pytorch autograd function\n",
    "import torch\n",
    "\n",
    "x=torch.tensor([1,2,3,4],dtype=torch.float32) #initialzie vars\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "w=torch.rand((1),dtype=torch.float32, requires_grad=True) #initialize weights, we are interested in its gradient\n",
    "\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "def loss(y,y_pred):\n",
    "    return ((y_pred-y)**2).mean() #MSE\n",
    "\n",
    "lr=0.01\n",
    "iters=301\n",
    "\n",
    "for epoch in range(iters):\n",
    "    y_pred=forward(x)\n",
    "    l=loss(y,y_pred)\n",
    "    \n",
    "    l.backward()           #gets the gradient of the loss wrt to the weights\n",
    "    \n",
    "    with torch.no_grad():  #Need to include this as we do not need grad calculation in updates of weights\n",
    "        w-=lr*w.grad       #adjust the weights\n",
    "        \n",
    "    w.grad.zero_()         #Need to reset the value of the gradient after each epoch\n",
    "    \n",
    "    if epoch % 50 == 0:    #Only print out epochs that are multiples of 20\n",
    "        \n",
    "        print(\"Epoch Number: \", epoch, \" Weight: \",w.item(), \" Loss: \", l.item()) #item only works if 1 item, if many use .data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number:  0  Weight:  0.4122750163078308  Loss:  26.168209075927734\n",
      "Epoch Number:  50  Weight:  1.9995304346084595  Loss:  2.2889221327204723e-06\n",
      "Epoch Number:  100  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  150  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  200  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  250  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  300  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  350  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  400  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  450  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n",
      "Epoch Number:  500  Weight:  1.9999996423721313  Loss:  8.988365607365267e-13\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When X = 5, Y = 9.999998092651367\n"
     ]
    }
   ],
   "source": [
    "#We can further optimize this model by calling the optimizer function in pytorch to calculate the loss for us\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=torch.tensor([1,2,3,4],dtype=torch.float32) #initialzie vars\n",
    "y=torch.tensor([2,4,6,8],dtype=torch.float32)\n",
    "\n",
    "w=torch.rand((1),dtype=torch.float32, requires_grad=True) #initialize weights, we are interested in its gradient\n",
    "\n",
    "def forward(x):\n",
    "    return w*x\n",
    "\n",
    "loss=F.mse_loss          #We used the torch.nn.functional MSE loss instead of calculating it ourselves\n",
    "\n",
    "lr=0.01\n",
    "iters=501\n",
    "errors=[]\n",
    "epochs=[]\n",
    "\n",
    "for epoch in range(iters):\n",
    "    y_pred=forward(x)\n",
    "    l=loss(y,y_pred)       #This part is still the same even for the functional MSE loss\n",
    "    \n",
    "    l.backward()           \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        w-=lr*w.grad       \n",
    "        \n",
    "    w.grad.zero_()         \n",
    "    \n",
    "    errors.append(l.item())  #appending the loss to the error list\n",
    "    epochs.append(epoch)     #appending the epochs to the epochs list\n",
    "    \n",
    "    if epoch % 50 == 0:    \n",
    "        \n",
    "        print(\"Epoch Number: \", epoch, \" Weight: \",w.item(), \" Loss: \", l.item())\n",
    "    \n",
    "plt.plot(errors)    #plot out how the loss function changes with time\n",
    "plt.show()\n",
    "\n",
    "print(\"When X = 5, Y =\",forward(5).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 3])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instead of having to define the function, we can use simple pre-built ones from pytorch nn \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "inputs=torch.tensor([73,67,43,91,88,64,87,134,58,102,43,37,69,96,70,73,67,43,91,88,64,87,134,58,102,43,37,69,96,70,73,67,43,91,88,64,87,134,58,102,43,37,69,96,70],dtype=torch.float32)\n",
    "inputs=inputs.view(-1,3)\n",
    "print(inputs.size())\n",
    "\n",
    "targets=torch.tensor([56,70,81,101,119,133,22,37,103,119,56,70,81,101,119,133,22,37,103,119,56,70,81,101,119,133,22,37,103,119],dtype=torch.float32)\n",
    "targets=targets.view(-1,2)\n",
    "print(targets)\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "trainds=TensorDataset(inputs,targets) #you feed TensorDataset the input and target dataset \n",
    "                                      #returns the pairs of input and target data in the form of tuples\n",
    "    \n",
    "    \n",
    "trainds[[1,3,5,7]]                    #Retrieving items with these index from the original dataset\n",
    "trainds[0:3]                          #Retrieve first 3 rows of input and target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69.,  96.,  70.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 87., 134.,  58.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [119., 133.]])\n"
     ]
    }
   ],
   "source": [
    "#Use torch's DataLoader\n",
    "#Splits the data into batches of predefined size, can do shuffling and random sampling\n",
    "\n",
    "from torch.utils.data import DataLoader  \n",
    "\n",
    "batch_size=5\n",
    "traindl=DataLoader(trainds, batch_size, shuffle=True)   #DataLoader takes in batches of tuples from TensorDataset\n",
    "\n",
    "for xb,yb in traindl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break       #Only see the first batch of inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6511.67626953125\n"
     ]
    }
   ],
   "source": [
    "#Instead of writing out the linear formula, we can use pytorch's nn.Linear function\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model=nn.Linear(3,2)             #nn.Linear(inputs features, outputs features), randomizes initial weights and biases\n",
    "                                 #our data is 15x3 15 data points, 3 parameters\n",
    "# print((model.weight))\n",
    "# print(model.bias)\n",
    "\n",
    "#print(list(model.parameters()))  #Does the same thing as printing the weights and bias\n",
    "\n",
    "pred=model(inputs)                #We feed the model the inputs\n",
    "#print(pred)\n",
    "loss=F.mse_loss \n",
    "\n",
    "print(loss(pred,targets).item())  #We calculate the loss in our model prediction and the true value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instead of manually manipulating the model's weights and biases using gradient descent\n",
    "#We can use the optimizer optim.SGD\n",
    "\n",
    "opt=torch.optim.SGD(model.parameters(), lr=0.00001)  #Optim takes in the model parameters, learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model with the same process\n",
    "\n",
    "1. Generate Prediction\n",
    "2. Calculate the Loss\n",
    "3. Compute Gradient wrt to weight and biases\n",
    "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
    "5. Reset the gradients to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  4435.55078125\n",
      "Epoch:  10 Loss:  455.6368103027344\n",
      "Epoch:  20 Loss:  194.05718994140625\n",
      "Epoch:  30 Loss:  51.27599334716797\n",
      "Epoch:  40 Loss:  94.11598205566406\n",
      "Epoch:  50 Loss:  77.73578643798828\n",
      "Epoch:  60 Loss:  82.37474060058594\n",
      "Epoch:  70 Loss:  53.32758331298828\n",
      "Epoch:  80 Loss:  27.34716796875\n",
      "Epoch:  90 Loss:  23.38990592956543\n",
      "tensor([[ 58.4779,  70.8996],\n",
      "        [ 84.6135,  98.2648],\n",
      "        [111.3765, 137.3514],\n",
      "        [ 28.6478,  40.6020],\n",
      "        [101.6733, 112.8012],\n",
      "        [ 58.4779,  70.8996],\n",
      "        [ 84.6135,  98.2648],\n",
      "        [111.3765, 137.3514],\n",
      "        [ 28.6478,  40.6020],\n",
      "        [101.6733, 112.8012],\n",
      "        [ 58.4779,  70.8996],\n",
      "        [ 84.6135,  98.2648],\n",
      "        [111.3765, 137.3514],\n",
      "        [ 28.6478,  40.6020],\n",
      "        [101.6733, 112.8012]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    \n",
    "    #loop for epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        #Train with batches of data\n",
    "        for xb,yb in traindl:\n",
    "            \n",
    "            #Generate Prediction\n",
    "            pred=model(xb)\n",
    "            \n",
    "            #Calculate the Loss\n",
    "            l=loss(pred,yb)\n",
    "            \n",
    "            #Compute gradient\n",
    "            l.backward()\n",
    "            \n",
    "            #Update parameters using gradients\n",
    "            opt.step()                                 #Weights are updated in opt.step, no more w-=lr*w.grad\n",
    "            \n",
    "            #Reset gradient to zero\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        if (epoch)%10==0:\n",
    "            print(\"Epoch: \",epoch, \"Loss: \", l.item())\n",
    "            \n",
    "fit(100, model, loss, opt)\n",
    "\n",
    "print(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 Loss:  40.033790588378906\n",
      "Epoch:  50 Loss:  2.504578113555908\n",
      "Epoch:  100 Loss:  0.6076387763023376\n",
      "Epoch:  150 Loss:  0.19098444283008575\n",
      "Epoch:  200 Loss:  0.06361020356416702\n",
      "Epoch:  250 Loss:  0.09330252557992935\n",
      "Epoch:  300 Loss:  0.02680574357509613\n",
      "Epoch:  350 Loss:  0.012361934408545494\n",
      "Epoch:  400 Loss:  0.0036960337311029434\n",
      "Epoch:  450 Loss:  0.0009362015989609063\n",
      "tensor([2.0040, 4.0162, 6.0041, 7.9948, 9.9678], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Summary of Using mini batch and SGD with Optimizer.step() and l.backward()\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "inputs=torch.tensor([1,2,3,4,5],dtype=torch.float32)\n",
    "targets=torch.tensor([2,4,6,8,10],dtype=torch.float32) \n",
    "\n",
    "trainbatch=TensorDataset(inputs,targets)                     #Pairs input and output into tuples\n",
    "\n",
    "batch_size=5                                                 #Set batch size to 5\n",
    "traindl=DataLoader(trainbatch, batch_size, shuffle=True )    #Gets mini batches of size 5 and shuffle\n",
    "\n",
    "model=nn.Linear(5,5)                                         #Linear regression model\n",
    "\n",
    "# pred=model(inputs) \n",
    "loss=F.mse_loss\n",
    "\n",
    "opt=torch.optim.SGD(model.parameters(), lr=0.01)             #define optimizer\n",
    "\n",
    "def fit(epoch, model, loss_fun, opt):                        #function to tune weights of the model\n",
    "    \n",
    "    for epoch in range(epoch):\n",
    "        \n",
    "        for xb,yb in traindl:                                 #for each batch in the data\n",
    "            \n",
    "            pred=model(xb)\n",
    "            \n",
    "            l=loss(pred,yb)\n",
    "            \n",
    "            l.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            opt.zero_grad()\n",
    "          \n",
    "        if epoch%50==0:  \n",
    "            print(\"Epoch: \", epoch, \"Loss: \",l.item())\n",
    "    \n",
    "fit(500,model,loss,opt)\n",
    "print(model(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
